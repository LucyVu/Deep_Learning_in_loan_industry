# -*- coding: utf-8 -*-
"""ADA Assignment_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jraYU5pw86VAcyBjpu-u1OClp7gENHcb
"""

# Import all the necessary libraries
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix,precision_score, recall_score,roc_curve, auc,roc_auc_score
import matplotlib.pyplot as plt
from keras import losses
import seaborn as sns

# Reading the file

loan_data = pd.read_excel('loan_data_ADA_assignment.xlsx')

# PART 1: DATA PREPARATION

# Check the structure of data set
loan_data.info()

# Identifying duplicate rows. There is no duplicate row in loan data set
num_duplicate_rows = loan_data.duplicated().sum()
print("Number of duplicate rows:", num_duplicate_rows)

# Checking for missing values
total_na = loan_data.isna().sum().sum()
print(total_na)

# Checking for missing values by column
na_counts_per_column = loan_data.isna().sum()
print(na_counts_per_column)

# Converting Mnths since columns to binary
loan_data['delinquency'] = np.where(loan_data['mths_since_last_delinq'].notna(), 1, 0)
loan_data['derogatory'] = np.where(loan_data['mths_since_last_major_derog'].notna(), 1, 0)
loan_data['record'] = np.where(loan_data['mths_since_last_record'].notna(), 1, 0)

# Get unique addr_state names
unique_addr_state = loan_data['addr_state'].unique()

# Print the list of unique_addr_state
print(unique_addr_state)

# Define regions
regions = {
    'Northeast': ['ME', 'NH', 'VT', 'MA', 'RI', 'CT', 'NY', 'NJ', 'PA'],
    'Midwest': ['OH', 'MI', 'IN', 'WI', 'IL', 'MN', 'IA', 'MO', 'ND', 'SD', 'NE', 'KS'],
    'South': ['DE', 'MD', 'DC', 'VA', 'WV', 'NC', 'SC', 'GA', 'FL', 'KY', 'TN', 'MS', 'AL', 'OK', 'TX', 'AR', 'LA'],
    'West': ['ID', 'MT', 'WY', 'CO', 'NM', 'AZ', 'UT', 'NV', 'CA', 'OR', 'WA', 'AK', 'HI']}

# Mapping function from state to region
def map_state_to_region(state):
    for region, states in regions.items():
        if state in states:
            return region

# Adding a new 'Region' column to the loan_data DataFrame
loan_data['region'] = loan_data['addr_state'].apply(map_state_to_region)

# Check value in region column, which is including 'South','Northeast','Midwest','West'
unique_regions = loan_data['region'].unique()
print(unique_regions)

#Dropping noise columns, High NA values and columns not suitable for one-hot encoding)
columns_to_drop = ["next_pymnt_d", "mths_since_last_delinq", "mths_since_last_major_derog",
                   "mths_since_last_record", "tot_coll_amt", "tot_cur_bal", "total_credit_rv",
                   "id", "member_id", "emp_title", "pymnt_plan", "desc", "title",
                   "policy_code", "zip_code", "addr_state"]

loan_data.drop(columns=columns_to_drop, inplace=True)

# Checking for missing values again
total_na = loan_data.isna().sum().sum()
print(total_na)

# Since the number of missing values are less than 5% of the dataset, we can remove them.
loan_data.dropna(inplace=True)

# Converting categorical variables to 'category' type
loan_data['grade'] = loan_data['grade'].astype('category')
loan_data['sub_grade'] = loan_data['sub_grade'].astype('category')

## Label encoding
# Ensure grade, home_ownership, verification_status are category with the specified levels
grade_categories = ["A", "B", "C", "D", "E", "F", "G"]

# Convert the category to ordered categories
loan_data['grade'] = pd.Categorical(loan_data['grade'], categories=grade_categories, ordered=True)

# Replace categories with their numerical codes
loan_data['grade'] = loan_data['grade'].cat.codes+1

# sub_grade
# Create an ordered category with levels
loan_data['sub_grade'] = pd.Categorical(loan_data['sub_grade'], ordered=True)
loan_data['sub_grade'] = loan_data['sub_grade'].cat.codes+1

## One-Hot encoding on 'purpose','loan_status','home_ownership','verification_status','region'
loan_data = pd.get_dummies(loan_data, columns=['purpose', 'loan_status',
                                               'home_ownership','verification_status','region'
                                               ], drop_first=True, dtype=int)

# Converting dates into days from 1st January 2024
dates = ['issue_d', 'last_pymnt_d', 'earliest_cr_line', 'last_credit_pull_d']

for column in dates:
    # Convert month-year strings to datetime, assuming the first day of the month
    loan_data[column] = pd.to_datetime(loan_data[column], format='%b-%y', errors='coerce')

    # Calculate the number of days from a reference date
    loan_data[column] = (pd.Timestamp('2024-01-01') - loan_data[column]).dt.days

# Standardising the Data
scaler = StandardScaler()
loan_data_scaled = scaler.fit_transform(loan_data)
data = pd.DataFrame(loan_data_scaled, columns=loan_data.columns,index=loan_data.index)
data['loan_is_bad'] = loan_data['loan_is_bad']

# Converting the loan_is_bad column to binary
data['loan_is_bad'] = loan_data['loan_is_bad'].astype(int)

# Check the value of Target variable
print(data['loan_is_bad'].head())

#Show bit of data
data.head()

#Separate the target column from the data
var_target = data.pop('loan_is_bad')

# Check data size again
print(len(data))

# PART 2: DATA PARTITIONING

#Split the data set into 80% training and 20% test set
from sklearn.model_selection import train_test_split
train_val_data, test_data, train_val_labels, test_labels = train_test_split(data,
                 var_target,
                 test_size=0.2,
                 stratify=var_target,
                 random_state=42)

# Split the training and validation set into 70% training and 10% validation

from sklearn.model_selection import train_test_split
train_data, val_data, train_labels, val_labels = train_test_split(train_val_data,
                 train_val_labels,
                 test_size=0.125,  # 0.125 x 0.8 = 0.1
                 stratify=train_val_labels,
                 random_state=42)

# Check the class distribution in the target column for loan_data_scaled, training, validation and test data set

# Data dataset
class_distribution = var_target.value_counts()
print("Training Set Class Distribution:\n", class_distribution)

class_distribution_percetage = var_target.value_counts(normalize=True) * 100
print("Class distribution in the data dataset:")
print(class_distribution_percetage)

# Training data set
# For absolute counts
train_class_distribution = train_labels.value_counts()
print("Training Set Class Distribution:\n", train_class_distribution)

# For percentages
train_class_distribution_percent = train_labels.value_counts(normalize=True)
print("Training Set Class Distribution Percentage:\n", train_class_distribution_percent)

# Validation data set
# For absolute counts
val_class_distribution = val_labels.value_counts()
print("Validation Set Class Distribution:\n", val_class_distribution)

# For percentages
val_class_distribution_percent = val_labels.value_counts(normalize=True)
print("Validation Set Class Distribution Percentage:\n", val_class_distribution_percent)

# Test data set
# For absolute counts
test_class_distribution = test_labels.value_counts()
print("Test Set Class Distribution:\n", test_class_distribution)

# For percentages
test_class_distribution_percent = test_labels.value_counts(normalize=True)
print("Test Set Class Distribution Percentage:\n", test_class_distribution_percent)

# Visualizing Class Distribution


def plot_class_distribution(labels, title):
    labels.value_counts(normalize=True).plot(kind='bar')
    plt.title(title)
    plt.xlabel('Class')
    plt.ylabel('Proportion')
    plt.xticks(rotation=0)  # Keeps the class labels horizontal
    plt.show()

plot_class_distribution(train_labels, 'Training Set Class Distribution')
plot_class_distribution(val_labels, 'Validation Set Class Distribution')
plot_class_distribution(test_labels, 'Test Set Class Distribution')

# Use oversampling to deal with data imbalance

#from imblearn.over_sampling import SMOTE

# Initialize SMOTE
#smote = SMOTE(random_state=42)

# Apply SMOTE to the training data only
#train_data, train_labels = smote.fit_resample(train_data, train_labels)

# Verify the class distribution is now 50/50
#print("After SMOTE - Counts of label '1':", sum(train_labels == 1))
#print("After SMOTE - Counts of label '0':", sum(train_labels == 0))

#print("\nClass distribution after SMOTE:")
#print(train_labels.value_counts(normalize=True))

# PART 3: DEEP LEARNING MODEL
# Convert the training and test sets to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((train_data.values, train_labels.values))
val_dataset = tf.data.Dataset.from_tensor_slices((val_data.values, val_labels.values))
test_dataset = tf.data.Dataset.from_tensor_slices((test_data.values, test_labels.values))

#Loops to optimise hyperparameters and generate training vs validation metric plots
# Define different parameters to iterate over
optimizers = [tf.keras.optimizers.AdamW]
loss_functions = [tf.keras.losses.BinaryCrossentropy]
metrics = [['accuracy', 'Precision', 'Recall']]
epochs_range = [100,200,250]  # Max 100 epochs
layer_width1 = [10,20,30,40,50,60]
layer_width2 = [5,10,20,30]
dropouts = [0,0.2,0.5]
batch_size = [50,100]

# This part we have run to check different combination of parameters and it is working but it takes long time to work
# # Iterate over parameter combinations
# for epochs in epochs_range:
#     for l1 in layer_width1:
#             for l2 in layer_width2:
#                     for batch_size in batch_size:
#                         for dropout_rate in dropouts:
#                             # Create modelRR
#                             #set batch size
#                             train_batch = train_dataset.batch(batch_size) # batch size = 50
#                             features, labels = next(iter(train_batch)) # iterate through each batch at training time
#                             model = tf.keras.Sequential([
#                                 tf.keras.layers.Dense(l1, activation=tf.nn.relu, input_shape=(len(train_data.columns), )),
#                                 tf.keras.layers.Dense(l2, activation=tf.nn.relu),
#                                 tf.keras.layers.Dropout(dropout_rate),
#                                 tf.keras.layers.Dense(1, activation='sigmoid')
#                                 ])

#                             # Compile model with selected optimizer, loss function, and metrics
#                             model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.005),
#                                           loss=tf.keras.losses.BinaryCrossentropy(),
#                                           metrics=['accuracy', 'Precision', 'Recall'])

#                             # Train the model
#                             historycheck = model.fit(features, labels,
#                                                      epochs=epochs,
#                                                      batch_size=batch_size,
#                                                      validation_data=(val_data, val_labels),
#                                                      verbose=0)  # Set verbose=0 to suppress output
#                             # Create a new figure
#                             fig = plt.figure(figsize=(10, 10))

#                             # Add overall title

#                             fig.suptitle(f"Dropout : {dropout_rate} \n  Epochs: {epochs} | Layer 1 Width : {l1} | Layer 2 Width : {l2} | Batch Size: {batch_size} \n Validation Accuracy: {round(historycheck.history['val_accuracy'][-1],2)} | Validation Precision: {round(historycheck.history['val_Precision'][-1],2)} | Validation Recall: {round(historycheck.history['val_Recall'][-1],2)}" , fontsize=16)

#                             # Subplot for loss
#                             plt.subplot(2, 2, 1)
#                             plt.plot(historycheck.history['loss'], label='loss')
#                             plt.plot(historycheck.history['val_loss'], label='val_loss')
#                             plt.xlabel('Epoch')
#                             plt.ylabel('Loss')
#                             plt.ylim([0, 1])
#                             plt.legend(loc='lower right')

#                             # Subplot for accuracy
#                             plt.subplot(2, 2, 2)
#                             plt.plot(historycheck.history['accuracy'], label='Accuracy')
#                             plt.plot(historycheck.history['val_accuracy'], label='val_accuracy')
#                             plt.xlabel('Epoch')
#                             plt.ylabel('Accuracy')
#                             plt.ylim([0, 1])
#                             plt.legend(loc='lower right')

#                             # Subplot for precision
#                             plt.subplot(2, 2, 3)
#                             plt.plot(historycheck.history['Precision'], label='Precision')
#                             plt.plot(historycheck.history['val_Precision'], label='val_precision')
#                             plt.xlabel('Epoch')
#                             plt.ylabel('Precision')
#                             plt.ylim([0, 1])
#                             plt.legend(loc='lower right')

#                             # Subplot for recall
#                             plt.subplot(2, 2, 4)
#                             plt.plot(historycheck.history['Recall'], label='Recall')
#                             plt.plot(historycheck.history['val_Recall'], label='val_recall')
#                             plt.xlabel('Epoch')
#                             plt.ylabel('Recall')
#                             plt.ylim([0, 1])
#                             plt.legend(loc='lower right')

#                             # Show the plot
#                             plt.tight_layout()
#                             plt.subplots_adjust(top=0.9)
#                             plt.show()




### Setting optimal parameters
batch_size = 50
epochs=200
dropout = 0.5
layer1_width = 50
layer2_width = 5

#Set batch size for training dataset
train_batch = train_dataset.batch(batch_size)
features, labels = next(iter(train_batch))
 # iterate through each batch at training time


#set batch size
train_batch = val_dataset.batch(len(val_dataset)) # batch size = 50
val_features, val_labels = next(iter(train_batch))

#Input layer, width and depth for the model
model = tf.keras.Sequential([
  tf.keras.layers.Dense(layer1_width, activation=tf.nn.relu, input_shape=(len(train_data.columns), )),
  tf.keras.layers.Dropout(dropout),
  tf.keras.layers.Dense(layer2_width, activation=tf.nn.relu), ## copy and paste below to add layer
  tf.keras.layers.Dense(1, activation = 'sigmoid')
])

#Setting optimiser, loss function and metrics for the model
model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001,  weight_decay=0.005),
              loss=tf.keras.losses.BinaryCrossentropy(), ### loss function
              metrics=(['Precision','Recall', 'accuracy'])) ### metric

#Fitting the model
historycheck = model.fit(features, labels, epochs=epochs, validation_data=(val_data, val_labels))

#Plot graphs for training vs validation metrics for loss, accuracy, precision and recall
# Create a new figure
fig = plt.figure(figsize=(10, 10))

# Add overall title

fig.suptitle(f"Dropout : 0.5 \n  Epochs: {epochs} | Layer 1 Width : {layer1_width} | Layer 2 Width : {layer2_width} | Batch Size: {batch_size} \n Validation Accuracy: {round(historycheck.history['val_accuracy'][-1],2)} | Validation Precision: {round(historycheck.history['val_Precision'][-1],2)} | Validation Recall: {round(historycheck.history['val_Recall'][-1],2)}" , fontsize=16)

# Subplot for loss
plt.subplot(2, 2, 1)
plt.plot(historycheck.history['loss'], label='loss')
plt.plot(historycheck.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')

# Subplot for accuracy
plt.subplot(2, 2, 2)
plt.plot(historycheck.history['accuracy'], label='Accuracy')
plt.plot(historycheck.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

# Subplot for precision
plt.subplot(2, 2, 3)
plt.plot(historycheck.history['Precision'], label='Precision')
plt.plot(historycheck.history['val_Precision'], label='val_Precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.ylim([0, 1])
plt.legend(loc='lower right')

# Subplot for recall
plt.subplot(2, 2, 4)
plt.plot(historycheck.history['Recall'], label='Recall')
plt.plot(historycheck.history['val_Recall'], label='val_Recall')
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.ylim([0, 1])
plt.legend(loc='lower right')

# Show the plot
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

# PART 4: EVALUATION PART



#feeding the entire test data into the model at once
test_batch = test_dataset.batch(len(test_data)) # -- the size of the the whole dataset
test_features, test_labels = next(iter(test_batch))

#Evaluate test metrics
test_loss, test1, test2, test3 = model.evaluate(test_features,  test_labels, verbose=2)

#Original number of positives
#sum(test_labels)
#Predictons
test_predictions = model.predict(test_features)
##converting probability predictions into binary
test_binary_predictions = [1 if pred >= 0.5 else 0 for pred in test_predictions]


#Create confusion matrix
confusion_test = confusion_matrix(test_binary_predictions,test_labels)

test_TN,test_FN, test_FP, test_TP = confusion_test.ravel()

#confusion_test

# Check for element in confusion matrix
test_FN

# Visualise confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_test, annot=True, fmt="d", cmap="Blues",
            yticklabels=['Predicted Negative', 'Predicted Positive'],
            xticklabels=['Actual Negative', 'Actual Positive'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Calculate metrics
test_accuracy = (test_TP + test_TN) / (test_TP + test_TN + test_FP + test_FN)
test_precision = test_TP / (test_TP + test_FP)
test_recall = test_TP / (test_TP + test_FN)
test_f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)

# Print metrics
print(f"test_accuracy: {test_accuracy}")
print(f"test_precision: {test_precision}")
print(f"test_recall: {test_recall}")
print(f"test_F1 Score: {test_f1_score}")

## combine test dataset + original target variable + prediction variables
test_data_results=pd.DataFrame(test_features.numpy())
test_data_results['loan_is_bad'] = test_labels  # add original_target column
test_data_results['predictions'] = test_binary_predictions  # add predictions column
test_data_results['Correct_Prediction'] = np.where(test_data_results['loan_is_bad'] == test_data_results['predictions'], 1, 0)

# ROC chart
# Compute ROC curve
fpr, tpr, thresholds = roc_curve(test_labels, model.predict(test_features))

# Compute ROC area under the curve
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# First, get the probabilities of the positive class
predicted_probs = model.predict(test_features).ravel()  # Flatten to 1D

# Calculate the AUC
auc_score = roc_auc_score(test_labels, predicted_probs)
print(f"AUC: {auc_score}")

# Generate an array of percentages to represent the x-axis from 0 to 100
percentages = np.linspace(0, 1, len(predicted_probs))

# Sort the probabilities and the corresponding labels
sorted_indices = np.argsort(predicted_probs)[::-1]
sorted_labels = np.array(test_labels)[sorted_indices]

# Calculate the cumulative gain
cumulative_gain = np.cumsum(sorted_labels) / np.sum(sorted_labels)

# Calculate the baseline (random model)
baseline = np.linspace(0, 1, len(cumulative_gain))

# Plot the Gain Chart
plt.figure(figsize=(8, 6))
plt.plot(percentages, cumulative_gain, label='Deep Learning Model', color='orange')
plt.plot(percentages, baseline, label='Baseline', linestyle='--', color='navy')
plt.xlabel('% Test Instances (Data)')
plt.ylabel('% Correct Predictions')
plt.title('Cumulative Gain Chart')
plt.legend(loc='lower right')
plt.show()



#full income test
# Convert the training and test sets to TensorFfull datasets
full_test_dataset = tf.data.Dataset.from_tensor_slices((data.values, var_target.values))
#feeding the entire test data into the model
test_batch = full_test_dataset.batch(len(full_test_dataset)) # -- the size of the the whole dataset
full_features, full_labels = next(iter(test_batch))

## this is the metrics that we used earlier for the model. For example right now, test1 is precision and test2 is recall. if you change the number of metrics you have to change the number of tests here as well can be called anything
full_test_loss, full_test1, full_test2, full_test3 = model.evaluate(full_features,  full_labels, verbose=2)

#Predictons
full_predictions = model.predict(full_features)
##converting probability predictions into binary
full_binary_predictions = [1 if pred >= 0.5 else 0 for pred in full_predictions]

## combine full dataset + original target variable + prediction variables
full_data_results=loan_data
full_data_results['predictions'] = full_binary_predictions  # add predictions column

# Create 'Predicted Correctly' column
full_data_results['Correct_Prediction'] = np.where(full_data_results['loan_is_bad'] == full_data_results['predictions'], 1, 0)

full_data_results.head()

#Income level generation
full_data_results['income_level'] = pd.qcut(full_data_results['annual_inc'], q=3, labels=['low', 'medium', 'high'])

# Bias analysis

# Create 'Predicted Correctly' column
full_data_results['Correct_Prediction'] = np.where(full_data_results['loan_is_bad'] == full_data_results['predictions'], 1, 0)

full_data_results.head()

full_data_results['income_level'] = pd.qcut(full_data_results['annual_inc'], q=3, labels=['low', 'medium', 'high'])


#Bias Analysis on income levels in the dataset
for i in loan_data['income_level'].unique():
    n_predictions_dl=len(full_data_results[(full_data_results['Correct_Prediction']==1) & (full_data_results['predictions']==1) & (full_data_results['income_level']==i)])
    n_actual= len(full_data_results[(full_data_results['loan_is_bad']==1) & (full_data_results['income_level']==i)])
    total= len(full_data_results[(full_data_results['income_level']==i)])
    print("Percentage of predictions of bad loans by model in",i ,'income category:',round((n_predictions_dl/total)*100,2))
    print("Percentage of actual bad loans in",i ,'income category:',round((n_actual/total)*100,2))

#Bias check in loan grade
for i in loan_data['grade'].unique():
    n_predictions_dl = len(full_data_results[((full_data_results['Correct_Prediction']==1) & full_data_results['predictions']==1) & (full_data_results['grade']==i)])
    n_actual= len(full_data_results[(full_data_results['loan_is_bad']==1) & (full_data_results['grade']==i)])
    total= len(full_data_results[(full_data_results['grade']==i)])
    print("Percentage of bad loan predictions by the model in loan grade",i ,":",round((n_predictions_dl/total)*100,2))
    print("Percentage of actual bad loans in loan grade",i,":",round((n_actual/total)*100,2))

#Bias check in Northeast region

n_predictions_dl=len(full_data_results[(full_data_results['Correct_Prediction']==1) & (full_data_results['predictions']==1) & (full_data_results['region_Northeast']==1)])
n_actual= len(full_data_results[(full_data_results['loan_is_bad']==1) & (full_data_results['region_Northeast']==1)])
total= len(full_data_results[(full_data_results['region_Northeast']==1)])
print("Percentage of predictions of bad loans by model in Northeast region",round((n_predictions_dl/total)*100,2))
print("Percentage of actual bad loans in  Northeast region",round((n_actual/total)*100,2))

#Bias check in South region

n_predictions_dl=len(full_data_results[(full_data_results['Correct_Prediction']==1) & (full_data_results['predictions']==1) & (full_data_results['region_South']==1)])
n_actual= len(full_data_results[(full_data_results['loan_is_bad']==1) & (full_data_results['region_South']==1)])
total= len(full_data_results[(full_data_results['region_South']==1)])
print("Percentage of predictions of bad loans by model in South region",round((n_predictions_dl/total)*100,2))
print("Percentage of actual bad loans in  South region",round((n_actual/total)*100,2))

#Bias check in West region
n_predictions_dl=len(loan_data[(full_data_results['Correct_Prediction']==1) & (full_data_results['predictions']==1) & (full_data_results['region_West']==1)])
n_actual= len(loan_data[(full_data_results['loan_is_bad']==1) & (full_data_results['region_West']==1)])
total= len(full_data_results[(full_data_results['region_West']==1)])
print("Percentage of predictions of bad loans by model in West region",round((n_predictions_dl/total)*100,2))
print("Percentage of actual bad loans in  West region",round((n_actual/total)*100,2))